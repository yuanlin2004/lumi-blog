---
layout: post
title: "llama-np.py (2): Bring up"
date: 2024-05-27
image: "/assets/lost.png"
excerpt: "bringup"
---


As I mentioned I the [first blog post]({{ site.baseurl }}{% post_url 2024-05-19-The-Fun-of-Re-Discovery %}) of `llama-np.py`, the development of `llama-np.py` progressed through four phases. 
1. Functional: Llama2 text completion
2. Speed: CPU performance tuning
3. Functional: Llama3 text completion, chat and sampler
4. Speed: CPU and GPU performance tuning 

I'd like to use several blog posts to describe interesting challenges and discoveries I encountered during development. This post focuses on the first phase.


## High Level Flow and Components 

The goal of llama-np.py is to perform inference of non-trivial language models like Llama-2-7B. The basic inference feature is text completion. So the first concrete task was to make llama-np.py perform end-to-end text completion using Llama-2-7B weights. My previous work mostly centered around the Transformer network, which is just one piece in the middle of the end-to-end flow.

<img style="float:left;" src="{{ site.baseurl }}/assets/llama-high-level-blocks.svg" width=300 hspace=10/> 

The left diagram I created shows this flow and the high level components. The input text string is fed into a **tokenizer** which translates/encodes the string into a sequence of tokens. A **language model** (e.g. Transformer) takes in the input tokens and generates logits, which is a vector of the size of the token table, with each value (corresponding to a token) representing how likely the token would be the next one. A **sampler** takes in the logits and picks the next token based some strategy/heuristics. Then the winning token is translated/decoded by the same **tokenizer** into an output string.  The flow up to this point is called the **pre-fill phase**. The language model can be made stateful to store the past history which is used for the subsequent token generation. This pre-fill phase fills the history buffer (a.k.a **kv-cache**) with results computed from the input prompt and generates the first output token (a.k.a **first-token generation**). Then the language model continues generating more tokens one by one based on the past history and previous tokens generated, until some stopping token is generated. The phase of subsequent token generation is called the **generation phase**. 

I had limited knowledge about the tokenizer and the sampler in my previous work, because they are usually not the performance bottlenecks,  To reduce my initial effort, I decided to reuse Llama 2's tokenizer module (`sentencepiece`) and implement a greedy sampling strategy (i.e., always picking the token with the highest score). This decision allowed me to focus on the more glamorous aspects of the language model first. However, my limited understanding of the tokenizer and sampler meant I was unaware of their impact on the quality of text generation, leading to several moments of confusion. More on this later.

## The Llama Network Architecture and My Sandwich Approach to Implementation

I am a visual thinker, so I put together another diagram for the Llama network. While it omits some important nuances, like the group query attention and kv cache, I was able to mentally fill in those gaps easily. 

<img src="{{ site.baseurl }}/assets/llama-network.svg" width="70%"> 

While drawing the diagram above, I realized that the first embedding layer and the final LM head perform very similar tasks, albeit in reverse. The first embedding layer is essentially a table lookup, retrieving an embedding vector for a given token ID. Conversely, the LM head takes a vector and computes a score for each token using dot products. I wondered whether the values in the embedding weight and the LM head weight were actually very similar. So I checked - they are quite different in Llama2-7B. I didn't delve deeper into this as I was eager to start my implementation. (Later, I discovered a known technique called **weight-tying** which reuses the same weights for both layers). 

However, based on this observation, I decided to implement both ends (encoding/embedding and LM head/sampler/decoding) first and then add in the Transformer components piece by piece. As long as the shapes match, the network should run without crashing. During the developement, I always asked `llama-np.py` to complete the input text "It is easy" (and so I wished). I expected `llama-np.py` to produce garbage initially, but once all the components were in place, I anticipated that the magic light would turn on and something wise should come out. 

The single most difficult part in terms of coding is to keep track of the tensor dimensions. In my mind, each dimension of a tensor carries a physical meaning (e.g. batch size, sequence length, table height, etc). Seeing the intensions/meanings of the dimensions in the code makes writing and reading the program much easier.  To draw an analogy, it would be a nightmare if one could name variables `a1`, `a2`, `a3`, .... Currently I rely on annotations/comments in the code and side documents (note that I've put the tensor shapes in the diagram) to keep me sane. Can this be made automatic/enforced? **Dimension-type-checking** (e.g. [jaxtyping](https://github.com/patrick-kidger/jaxtyping)) is an interesting topic to explore next. 

## Bring Up

"After rounds of thorough modular testing, the day of bringup finally came". Well, because of the sandwich approach, it was actually like "I think I've everything in, but why 'It is easy ed ; a a a a a a a a a a a'?". 

So I pulled out my first cheat sheet. Andrej Karpathy's [llama2.c](https://github.com/karpathy/llama2.c) is an amazing project that implements the Llama2 inference using just C. It actually inspired my own `llama-np.py` and gave me the confidence to pursue it as a hobby project. Since both implement the Llama network, I realzied it would be possible to compare the intermediate results layer by layer. 

I converted the Llama-2-7B weights to the format llama2.c reads and inserted printf statements in llama2.c. Llama2.c feeds the tokens in the pre-fill stage one by one rather than as a sequence at one. I added a similar mode in `llama-np.py`. 

<p id="easy"></p>In the end, I had to fix only one bug in my RoPE implementation before the wisdom was revealed:

> It is easy to get lost in the world of the Internet. 

<img src="{{ site.baseurl }}/assets/lost.png" width="100%" />


Next post: ["Beef up"]({{ site.baseurl }}{% post_url 2024-05-28-beefup %})
