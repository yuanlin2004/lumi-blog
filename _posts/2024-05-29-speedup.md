---
layout: post
title: "llama-np.py (4): Speed up"
date: 2024-05-29
image: "/assets/nsys1.png"
excerpt: "speedup"
---


In the past, I've heard countless stories of 20x, 50x, 100x speed improvements, and it always warmed my heart to known I indirectly contributed to some of those. Now I have my own firsthand experience in recent years. While the techniques used weren't particularly fancy, there were a few unexpected twists that made the puzzle-solving process quite engaging. 

Due to the significant increase in speed, I'll use either seconds per token (sec/token) or tokens per second (token/sec) as the metric, depending on which provides a clearer comparison.

## Goal and Status

### Goal
My objective was to perform inference of the Llama-2-7B model in less than 7 sec/token on my gaming PC circa 2023, equipped with 

    - AMD Ryzen 7700x CPU 
    - 96GB DDR5 RAM
    - RTX 4070 GPU, 12GB VRAM
    - Ubuntu 22.04

The 7-second mark is a common threshold for response time in computer-human interaction. Having previously experimented with [Ollama](https://github.com/ollama/ollama), which achieved about 10 tok/sec on a MacBook Pro M2 using 8-bit quantization, I believed achieving 7 sec/token with fp32 on my PC should be attainable. 

### Status

Original speed when started: 45 sec/token

Currently achieved speed: 0.5 sec/token


## From 45 sec/tok to 6 sec/tok: Tensor with a View

The initial `llama-np.py` managed 42 tok/sec with TinyStories-15M model, but with the Llama-2-7B, the speed plummeted to 45 sec/token - far from my 7 sec/token goal. During the [bringup]({% post_url 2024-05-27-bringup %}#easy), I made it possible to run `llama2.c` with the same Llama-2-7B model on my PC. So I measured its speed - it was 5 sec/token. `llama2.c` was a straightforward C implementation with very few optimizations, so the performance difference couldn't be due to the use of NumPy alone. Something was clearly off. 

I added a [per-function timing feature](https://github.com/yuanlin2004/lumi/blob/main/llama-np/decotimer.py) using Python decorators. As expected, most time was spent in the Linear layers. I realized I knew very little about NumPy and decided it was time to read up on it. 

Skimming through Travis E. Oliphant's '[Guide to Numpy](https://web.mit.edu/dvp/Public/numpybook.pdf)', I figured out the issue. Here was how the Linear layer was implemented. 

```python
class Linear:
    def __init__(self, weight, bias=None):
        self.weight, self.bias = weight.T, bias

    def __call__(self, x):
        y = np.matmul(x, self.weight)
        return y + self.bias if self.bias is not None else y
```

NumPy creates a **view** for the transpose `weight.T` in the `__init__()` function without changing the memory layout. The `matmul` function was very inefficient with the transposed layout. Changing `weight.T` to `weight.T.copy()` reduced the per-token generation time from 45 seconds to 6 seconds, a 7.5x speedup :).

I could have stopped here, having achieved my original 7 sec/token goal. But 7 seconds per token felt painfully slow, and `llama2.c` was still faster by one second. Something was still off. 


## From 6 sec/token to 0.8 sec/token: Adopting CuPy Helped Unexpectedly 

My goal was to further improve the speed using quick and easy methods. The idea of using CuPy popped up. 
 
- I had always been curious about CuPy and had never had a chance to play with it before. 
- There was a 4070 sitting idle in the box. 
- I was not sure using CuPy would help the speed or not. But hey, it was based on CUDA. I knew I would eventually come to it. Why not now?

Adding initial CuPy support was surprisingly straightforward:

```python
    global np
    if args.use_cupy:
        np = cupy
    else:
        np = numpy
```

For tiny models like TinyStories-15M, the CuPy version achieved 148 tok/s, outperforming the NumPy version's 94 tok/sec - not bad. However, for Llama2-7B, the 12GB of VRAM on 4070 was insufficient for all the weights. 7B of bf16 weights need roughly 14GB and I was using fp32. Using unified memory mitigated the out-of-memory issue but did not improve speed. I had to selectively offload a few layers to the GPU. The per-token generation time went down to 3 seconds - a 2x speedup by using CuPy. 

Once on the GPU side, I felt more at home. Tuning CUDA performance without checking the nsys profiling result is worse than banging one's head against a wall. My nsys profiling result had two surprises. First, many kernel names had fp64 in them, indicating the kernels were doing fp64 computations. Second, there were many 4-byte device to host (D2H) copies scattered across the timeline.  

I didn't think I had put any fp64 code in `llama-np.py`. Where did the fp64 kernels come from? Because there was no kernel fusion, it was quite easy to associate each kernel with its original CuPy operation. It turned out that, with its root in HPC, NumPy implicitly chooses int64 and fp64 types. For example, `numpy.arange()`'s data type is `Int64DType` and `-2.0` is a `Float64DType`. The following shows the changes I made in my `RoPE()` implementation to keep computations in fp32.

<img src="{{ site.baseurl }}/assets/fp32.png"/>

After closing all the fp64 holes, the token generation time for CuPy dropped from 3 sec/token to 2 sec/token. 

And the NumPy time went down from 6 sec/token to 0.8 sec/token!


## Reaching 0.5 sec/tok 

With NumPy version being 2.5x faster than the CuPy version,  I knew I could not stop here. 

Why were there so many small 4-byte D2H copies? The reason was quite obvious from hind sight. For debugging, I had inserted many logging calls using Python's `logging` module. For example, 

```python
logger.debug(f"att rmsnorm [50] {norm[0][50]}")
```

While the logging function would not be triggered when the log level was below `debug`, the argument evaluation (e.g. `norm[0][50]` which needed a D2H copy) still happened. The [fix](https://github.com/yuanlin2004/lumi/commit/cf8402d6358ad33e2ab2a3d2654b58db9e236620) was to implement my own logger leveraging Python's callable feature. 

I wanted to offload more layers to GPU to reduce CPU-GPU ping-pong. The bottle neck was limited memory size. I was not ready to go down the path of int8/int4 quantization yet. Could I do anything with bf16? The Llama weights were originally in bf16, and I converted them to fp32. Since bf16 is essentially fp32 with a truncated mantissa (bf16 was originally just a storage format), converting between fp32 and bf16 is simple, if we ignore rounding modes. By faking bf16 using int16, I could put almost 2x layers on the GPU. It also cut weight file size in half, significantly reducing weight loading time from the SSD. 

With these tricks and a few to get rid of some CPU bubbles, the token generation time using CuPy was reduced to 0.5 sec/token. Good enough, for now :)

<img src="{{ site.baseurl }}/assets/nsys1.png"/>


## Coming up

This concludes my story of how `llama-np.py` was initially developed. Moving forward, I'll share some interesting experiments, including a story about how [https://medium.com/@serenadesilva](https://medium.com/@serenadesilva) was born. 
