---
layout: post
title: "llama-np.py (3): Beef up"
date: 2024-05-28
image: "/assets/cmd.png"
excerpt: "beefup"
---


As I mentioned in the [first blog post]({{ site.baseurl }}{% post_url 2024-05-19-The-Fun-of-Re-Discovery %}) of `llama-np.py`, the development progressed through four key phases:

1. Functional: Llama2 text completion
2. Speed: CPU performance tuning
3. Functional: Llama3 text completion, chat and sampler
4. Speed: CPU and GPU performance tuning

The previous post ["Bring up"]({{ site.baseurl }}{% post_url 2024-05-27-bringup %}) focused on the first phase. This post will cover the rest of phase 1 and phase 3, leaving the speed improvements to the next post ["Speed up"]({{ site.baseurl }}{% post_url 2024-05-29-speedup %}). 

<img src="{{ site.baseurl }}/assets/cmd.png" width="100%" />

## Many Paths

In the days following my discovery of ["what was easy"]({{ site.baseurl }}{% link _posts/2024-05-27-bringup.md %}#easy), I added 

- a mode that does not use KV cache 
- two modes of allocating memory for KV cache - dynamically growing and statically fixed. 

Along with the extra mode for the pre-fill stage (for testing with `llama2.c` in the bringup phase)  and the CuPy mode added later, `llama-np.py` has 12 different paths that should produce the same output logits. Understanding the trade-offs and making these options work will be an interesting topic for another post. 


## More Models and the Lumi Weight Format 

A primary purpose of creating `llama-np.py` was to understand and experiment with different models. Consequently, adding support for more models became a priority.

[Currently @84b3832](https://github.com/yuanlin2004/lumi/tree/84b3832169e9b7ef4f1feaf9680bfba8acaff964), `llama-np.py` supports the following models.

| Model            | Size | Major differences from llama-2-7b |
| ---------------- | ---- | -------------------------|
| TinyStories      | 260k | use GQA                 |
| TinyStories      | 15M  | none     |
| TinyStories      | 42M  | none     |
| TinyStories      | 110M | none              |
| TinyLlama        | 1.1B | use GQA, HF permuted Q/K weights |
| TinyLlama-Chat   | 1.1B | use GQA, HF permuted Q/K weights |
| Llama-2          | 7B   | base (use MHA, use sentencepiece for tokenizer, no bias in Q/K/V, RoPE rotate pair) |
| Llama-2-Chat     | 7B   | base |
| Llama-3          | 8B   | use GQA, use tiktoken for tokenizer |
| Llama-3-Instruct | 8B   | use GQA, use tiktoken for tokenizer |
| QWen1.0-7B-Chat  | 7B   | use tiktoken for tokenizer, Q/K/V uses bias, Q/K/V next to each other in one weight tensor, RoPE rotate half          | 

The initial implementation of `llama-np.py` was overfit to llama-2-7B, with many hard-coded hyper-parameters. Llama-2-7B also turned out be too large for initial testing and play, taking about ~45 seconds to generate a single token. Therefore, several tiny models were added first. 

Meta released Llama-3 a few weeks after `llama-np.py` was functional. By then, I'd already added a few smaller models and generalized the codebase. Adding Llama-3-8B was easier than expected. Adding GQA support essentially boiled down to [four lines of code change](https://github.com/yuanlin2004/lumi/blob/e9684028e745a58db67589355e6809b4214e7052/llama-np/transformer.py#L319:#L322). Tiktoken is also much simpler than SentencePiece. 

I want to add at least one Chinese language model and I heard good words about QWen, so QWen I included it. 

Reading the original model files requires either PyTorch or the Hugging Face library. To avoid these dependencies and make the model files Python-agnostic, I created  `convert.py`, which translates the different model files into a Lumi Weight Foramt defined by myself. `convert.py` is probably the least interesting and the most hacky piece of code in this project. It has many baked-in knowledge of the different models, e.g. weight names, etc. But I can live with that. 


## Tokenizers

To simplify initial efforts, I used the same tokenizer (SentencePiece) that Meta uses for Llama-2. When Llama-3 came out, I added the Tiktoken library, which also works for the Qwen1.0 model. 
With the coming QWen1.5/2.0 model, I would probably add Hugging Face's tokenizer library. I don't like the current approach and would like to have my own implementation - for three reasons,

1. To minimize the external dependencies. 
2. Since I only need token encoding and decoding only right now, the implementation effort should be reasonable. 
3. Tokenization is still a semi-black hole in my knowledge map, and I am very curious about it. 

This is high on my to-do list. 


## Sampler

I was [thrilled]({% link _posts/2024-05-27-bringup.md %}#easy) to see `llama-np.py` finally emitting "It is easy to get lost in the world of the internet". But my joy was short-lived as the next string it generated was the same, repeated over and over. It seemed really lost. My initial suspicion was that my code failed to catch some special EOS (end-of-sequence) token, but I found nothing wrong. Then I tested the same prompt with `llama2.c` (with the temperature value setting to 0). It yielded the same repetitive result. I sensed I probably missed something really basic. 

The issue lay wit the sampler. To generate the next token, what the language model actually does is to assign a score to each and every token in the vocabulary. It is the sampler's job to pick the winning token. For simplicity, my initial `llama-np.py` always picked the one with the highest score. This approach is known to lead to deterministic and conservative outputs (this I knew) and "[repetitive and awkward](https://arxiv.org/abs/1904.09751)" results (this I did not know), especially with less well trained models. "[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)" was easy to read and I implemented the 
*Nucleus Sampling* (a.k.a topp) method in this paper. This method computes the probability distribution based on the scores computed by the language model, sorted the token according to their probabilities (high to low), find the smallest set of tokens (from the beginning) whose accumulated sum of probabilities exceeds a given parameter value *p* (say 0.95), and then randomly selects a token from this set. 

Here is an example output with the same `llama-2-7b` base model using the topp sampler.

"It is easy to think of a camera as just a camera, but the camera is a piece of equipment that can do a lot more than take photos. There are many ways that a camera can be used for purposes other than taking pictures. ..."

Later I found that sampler is another interesting field that draw lots of attentions. For example, some studied how to get the result that satisfy certain constraints (e.g. must have certain words). Some selects the final result based on some combined metrics of all the tokens in the generated sequence. Sampler plays an important role in the final quality of text generated. This also leads to some criticism on the current approach.  While it is a common practice now, I am not quite conformable with the idea of randomness. I added a few experimental features in `llama-np.py` to explore its impact, which could be an interest topic for a future post.

## Let's chat.

Adding chat mode involves learning a few know-hows, such as using special tokens like `<|im_start|>` to signify meta information in the input token stream. The most annoying part is that different models can have different [formats](https://huggingface.co/docs/transformers/main/en/chat_templating) for chat prompts. OpenAI even released and then [unreleased](https://github.com/openai/openai-python/pull/677) [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language) - a nice (IMO) Markup Language for structuring conversation turns.

Eager to see chat in motion, I took the shortcut of treating each turn of conversation as a new text completion task. All the previous conversations between the user and the model became the input prompts to a new text completion call. I saved the all the previous tokens so the encoding step can be skipped. While this approach reuses the existing text completion code with minimum changes, the time spent in generating the first token in each conversation becomes longer and longer as the conversation proceeds, because subsequent pre-fill stage needs to rebuild a larger KV-cache with more tokens. An alternative approach would be to use the same text completion session for all turns of conversations, storing past conversation turns in the KV cache. This would require handling a non-empty KV cache in the pre-fill stage. 

<img src="{{ site.baseurl }}/assets/chat.svg" width="100%"/>



## A compact version and 150 lines of code vs 13GB of weights

With the inclusion of above features, the `llama-np.py` code base has become overly complex, with excessive boilerplate obscuring the core Transformer logic. To address this, I created `compact-llama-np.py`, a streamlined version compatible with the `llama-np.py` models and maintains satisfactory performance.

`compact-llama-np.py` :
- supports the same set of models as `llama-np.py`. 
- has the same class structure as `llama-np.py`. 
- has the same CPU performance as `llama-np.py`.
- performs text completion but not chat.
- uses a simple greedy sampler 
- shares the same tokenizer and model reading modules with `llama-np.py`.

The core Transformer part has about [150 lines of code](https://github.com/yuanlin2004/lumi/blob/8b204d1fc3a1eaf861325401a39479259bba27ff/llama-np/compact-llama-np.py#L11-L151). The whole file is about 200 lines.  

<img src="{{ site.baseurl }}/assets/codevsw.svg" width="100%"/>

In the following days, I tested the model with various questions. Observing how well it responded, and knowing that there is only 150 lines of core code,  I was truly amazed by the seismic shift in software development. I heard that Tesla FSD v12 has dropped >300k lines of C++ control code by using neural networks. I hadn't fully internalize the impact until now. 

----

Well, there was one major issue I have not talked about yet. When using the llama-2-7b model, the initial `llama-np.py` needed 45 seconds to generate a single token - much longer than the 7 second limit I set. That's the topic of my next [post]({{ site.baseurl }}{% post_url 2024-05-29-speedup %}).



